from openai import OpenAI
import json
import os
import streamlit as st
import time
import logging
from typing import Optional, Dict, Any, Tuple
from enum import Enum

# é…ç½®æ—¥å¿—
import os
import logging

# æ ¹æ®ç¯å¢ƒè®¾ç½®æ—¥å¿—çº§åˆ«
if os.environ.get('STREAMLIT_ENV') == 'production':
    logging.basicConfig(level=logging.WARNING)  # ç”Ÿäº§ç¯å¢ƒåªæ˜¾ç¤ºè­¦å‘Šå’Œé”™è¯¯
else:
    logging.basicConfig(level=logging.INFO)     # å¼€å‘ç¯å¢ƒæ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯

logger = logging.getLogger(__name__)

class AIErrorType(Enum):
    """AIé”™è¯¯ç±»å‹æšä¸¾"""
    API_KEY_MISSING = "api_key_missing"
    API_KEY_INVALID = "api_key_invalid"
    NETWORK_ERROR = "network_error"
    RATE_LIMIT = "rate_limit"
    QUOTA_EXCEEDED = "quota_exceeded"
    MODEL_UNAVAILABLE = "model_unavailable"
    CONTENT_TOO_LONG = "content_too_long"
    UNKNOWN_ERROR = "unknown_error"

class AIProcessorError(Exception):
    """AIå¤„ç†å™¨è‡ªå®šä¹‰å¼‚å¸¸"""
    def __init__(self, error_type: AIErrorType, message: str, original_error: Optional[Exception] = None):
        self.error_type = error_type
        self.message = message
        self.original_error = original_error
        super().__init__(message)

class AIProcessor:
    """
    AIå¤„ç†å™¨ï¼Œè´Ÿè´£è°ƒç”¨ä¸åŒçš„AIæ¨¡å‹å¤„ç†æ–‡æœ¬
    æ”¯æŒçš„æ¨¡å‹ï¼šGPT-4o-mini, GPT-4o, DeepSeek, Claude-3
    """
    def __init__(self):
        """
        åˆå§‹åŒ–AIå¤„ç†å™¨
        """
        self.clients = self._initialize_clients()
        self.max_retries = 3
        self.retry_delay = 1  # ç§’
        
    def _initialize_clients(self) -> Dict[str, Any]:
        """åˆå§‹åŒ–å„ä¸ªAIå®¢æˆ·ç«¯"""
        clients = {}
        
        try:
            # OpenAIå®¢æˆ·ç«¯
            api_key = self._get_openai_api_key()
            if api_key:
                clients['openai'] = OpenAI(api_key=api_key)
                logger.info("OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.warning("OpenAI API key not found")
                
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–AIå®¢æˆ·ç«¯å¤±è´¥: {e}")
            st.error(f"AIæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            
        return clients
    
    def _get_openai_api_key(self) -> Optional[str]:
        """è·å–OpenAI APIå¯†é’¥"""
        try:
            # ä¼˜å…ˆä»session_stateè·å–
            if hasattr(st.session_state, 'api_key') and st.session_state.api_key:
                return st.session_state.api_key
            
            # ä»secretsè·å–
            if hasattr(st, 'secrets') and st.secrets.get('openai_api_key'):
                return st.secrets.get('openai_api_key')
                
            # ä»ç¯å¢ƒå˜é‡è·å–
            return os.getenv('OPENAI_API_KEY')
            
        except Exception as e:
            logger.error(f"è·å–APIå¯†é’¥å¤±è´¥: {e}")
            return None

    # def count_tokens(self, text):
    #     """è®¡ç®—æ–‡æœ¬çš„tokenæ•°é‡"""
    #     try:
    #         encoding = tiktoken.get_encoding("cl100k_base")
    #         return len(encoding.encode(text))
    #     except Exception as e:
    #         print(f"Error counting tokens: {e}")
    #         return 0

    def process_text(self, file_name: str, clean_lines: str, model: str = "gpt-4o", temperature: float = 0.3) -> Optional[str]:
        """
        å¤„ç†æ–‡æœ¬ï¼Œæ ¹æ®æŒ‡å®šçš„æ¨¡å‹è°ƒç”¨ç›¸åº”çš„API
        :param file_name: æ–‡ä»¶å
        :param clean_lines: è¦å¤„ç†çš„æ–‡æœ¬
        :param model: ä½¿ç”¨çš„æ¨¡å‹åç§°
        :param temperature: æ¸©åº¦å‚æ•°
        :return: å¤„ç†ç»“æœ
        """
        try:
            # éªŒè¯è¾“å…¥å‚æ•°
            self._validate_inputs(file_name, clean_lines, model, temperature)
            
            # æ£€æŸ¥å†…å®¹é•¿åº¦
            if len(clean_lines) > 100000:  # çº¦100kå­—ç¬¦
                raise AIProcessorError(
                    AIErrorType.CONTENT_TOO_LONG,
                    "å†…å®¹è¿‡é•¿ï¼Œè¯·å°è¯•åˆ†å‰²å¤„ç†"
                )
            
            # æ„å»ºæç¤ºè¯
            system_prompt, user_prompt = self._build_prompts(file_name, clean_lines)
            
            # æ ¹æ®æ¨¡å‹é€‰æ‹©å¤„ç†æ–¹æ³•
            if model.lower() == "gpt-4o":
                return self._process_with_retry(
                    self._process_with_gpt4o,
                    system_prompt, user_prompt, temperature
                )
            else:
                raise AIProcessorError(
                    AIErrorType.MODEL_UNAVAILABLE,
                    f"ä¸æ”¯æŒçš„æ¨¡å‹: {model}"
                )
                
        except AIProcessorError as e:
            self._handle_ai_error(e)
            return None
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡æœ¬æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            st.error(f"å¤„ç†å¤±è´¥: {str(e)}")
            return None
    
    def _validate_inputs(self, file_name: str, clean_lines: str, model: str, temperature: float):
        """éªŒè¯è¾“å…¥å‚æ•°"""
        if not file_name or not isinstance(file_name, str):
            raise ValueError("æ–‡ä»¶åä¸èƒ½ä¸ºç©º")
        
        if not clean_lines or not isinstance(clean_lines, str):
            raise ValueError("æ–‡æœ¬å†…å®¹ä¸èƒ½ä¸ºç©º")
        
        if not model or not isinstance(model, str):
            raise ValueError("æ¨¡å‹åç§°ä¸èƒ½ä¸ºç©º")
        
        if not isinstance(temperature, (int, float)) or temperature < 0 or temperature > 2:
            raise ValueError("æ¸©åº¦å‚æ•°å¿…é¡»åœ¨0-2ä¹‹é—´")
    
    def _build_prompts(self, file_name: str, clean_lines: str) -> Tuple[str, str]:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯å’Œç”¨æˆ·æç¤ºè¯"""
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é“¶è¡Œè´¦å•åˆ†æåŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼š
            1. å‡†ç¡®è¯†åˆ«å’Œæå–é“¶è¡Œè´¦å•ä¸­çš„äº¤æ˜“è®°å½•
            2. æ­£ç¡®åˆ†ç±»æ¯ç¬”äº¤æ˜“ï¼ˆæ”¶å…¥/æ”¯å‡ºï¼‰
            3. ç†è§£äº¤æ˜“æè¿°å¹¶è¿›è¡Œåˆé€‚çš„åˆ†ç±»
            4. ç¡®ä¿é‡‘é¢å’Œæ—¥æœŸçš„å‡†ç¡®æ€§
            5. éµå¾ªæŒ‡å®šçš„è¾“å‡ºæ ¼å¼
            è¯·ä¿æŒä¸“ä¸šã€å‡†ç¡®ï¼Œå¹¶ç¡®ä¿ä¸é—æ¼ä»»ä½•äº¤æ˜“è®°å½•ã€‚"""

        user_prompt = f"""
        è¯·ä»”ç»†åˆ†æä»¥ä¸‹é“¶è¡Œè´¦å•æ–‡æœ¬ï¼Œæå–æ‰€æœ‰äº¤æ˜“è®°å½•å¹¶æŒ‰ç…§æŒ‡å®šæ ¼å¼è¾“å‡ºã€‚

        é‡è¦è¯´æ˜ï¼š
        1. è¯·ä»æ–‡ä»¶å "{file_name}" ä¸­æå–å¹´ä»½ä¿¡æ¯ã€‚
        2. æ‰€æœ‰äº¤æ˜“è®°å½•å¿…é¡»ä½¿ç”¨è¯¥å¹´ä»½ï¼Œä¸è¦ä½¿ç”¨å…¶ä»–å¹´ä»½ã€‚
        3. å¿…é¡»å¤„ç†æ‰€æœ‰äº¤æ˜“è®°å½•ï¼Œç»å¯¹ä¸èƒ½é—æ¼ä»»ä½•ä¸€æ¡ï¼
        4. å¦‚æœå†…å®¹å¤ªé•¿ï¼Œè¯·ç¡®ä¿å¤„ç†å®Œæ‰€æœ‰å†…å®¹å†è¿”å›ï¼
        5. äº¤æ˜“è®°å½•ä¸­å³ä½¿å†…å®¹ç›¸ä¼¼ï¼Œä¹Ÿå¿…é¡»é€æ¡å®Œæ•´ä¿ç•™ï¼Œä¸èƒ½åˆå¹¶æˆ–çœç•¥ã€‚æ¯æ¡è®°å½•å¯é€šè¿‡ä½™é¢å˜åŒ–ç­‰ç»†èŠ‚è¿›è¡ŒåŒºåˆ†ã€‚

        æ–‡æœ¬å†…å®¹ï¼š
        {clean_lines}

        è¾“å‡ºè¦æ±‚ï¼š
        1. æ ¼å¼ï¼šæ—¥æœŸ | ç±»å‹ | é‡‘é¢ | ä¸€çº§åˆ†ç±» | äºŒçº§åˆ†ç±» | è´¦æˆ·1 | è´¦æˆ·2 | å¤‡æ³¨ | è´§å¸ | æ ‡ç­¾
        2. æ¯è¡Œä¸€æ¡äº¤æ˜“è®°å½•
        3. æ‰€æœ‰å­—æ®µç”¨ç«–çº¿ç¬¦"|"åˆ†éš”
        4. æ— å†…å®¹çš„æ ç›®ä¿æŒç•™ç©º
        5. ä¿ç•™Descriptionå­—æ®µåˆ°å¤‡æ³¨åˆ—ä¸­
        6. ä»ä¸Šä¸‹æ–‡ä¸äº¤æ˜“è®°å½•çš„å¤‡æ³¨ä¸­è·å–å¯¹åº”çš„é“¶è¡Œè´¦æˆ·åå››ä½
        7. ä¸è¦é—æ¼ä»»ä½•ä¸€æ¡äº¤æ˜“ä¿¡æ¯

        å¤„ç†è§„åˆ™ï¼š
        1. è´¦æˆ·ä¿¡æ¯ï¼šåœ¨è´¦æˆ·1å’Œè´¦æˆ·2å­—æ®µä¸­åŒ…å«è´¦æˆ·æœ€åå››ä½æ•°å­—ï¼Œç”¨æ‹¬å·æ‹¬èµ·ã€‚
        2. é‡‘é¢å¤„ç†ï¼šä¿æŒåŸå§‹é‡‘é¢çš„æ­£è´Ÿå€¼ï¼Œé‡‘é¢å¿…é¡»åŒ…å«å°æ•°ç‚¹å’Œä¸¤ä½å°æ•°ã€‚
        3. æ—¥æœŸæ ¼å¼ï¼šå¿…é¡»ä½¿ç”¨ä»æ–‡ä»¶åä¸­æå–çš„å¹´ä»½ï¼Œæ ¼å¼ï¼šYYYY-MM-DDã€‚
        4. åˆ†ç±»è§„åˆ™ï¼šç±»å‹æ å¯å¡«å…¥[è½¬è´¦, æ”¶å…¥, æ”¯å‡º]ï¼Œæ ¹æ®é‡‘é¢æ­£è´Ÿå’Œäº¤æ˜“æè¿°åˆ¤æ–­ã€‚
        5. äº¤æ˜“ç‹¬ç«‹æ€§ï¼šå³ä½¿äº¤æ˜“å†…å®¹ç›¸ä¼¼ï¼Œå¿…é¡»å®Œæ•´ä¿ç•™æ¯æ¡è®°å½•ã€‚
        6. è´§å¸æ ‡æ³¨ï¼šæ ¹æ®é“¶è¡Œç±»å‹æ ‡æ³¨USDæˆ–CNYã€‚

        é‡è¦æé†’ï¼š
        - å¿…é¡»å¤„ç†æ‰€æœ‰äº¤æ˜“è®°å½•ï¼Œç»å¯¹ä¸èƒ½é—æ¼ï¼
        - æ‰€æœ‰æ—¥æœŸå¿…é¡»ä½¿ç”¨ä»æ–‡ä»¶åä¸­æå–çš„å¹´ä»½ï¼
        """
        
        return system_prompt, user_prompt
    
    def _process_with_retry(self, process_func, *args, **kwargs) -> Optional[str]:
        """å¸¦é‡è¯•æœºåˆ¶çš„å¤„ç†æ–¹æ³•"""
        last_error = None
        
        for attempt in range(self.max_retries):
            try:
                logger.info(f"å°è¯•ç¬¬ {attempt + 1} æ¬¡å¤„ç†")
                result = process_func(*args, **kwargs)
                if result:
                    logger.info("å¤„ç†æˆåŠŸ")
                    return result
                    
            except AIProcessorError as e:
                last_error = e
                if e.error_type in [AIErrorType.RATE_LIMIT, AIErrorType.NETWORK_ERROR]:
                    if attempt < self.max_retries - 1:
                        wait_time = self.retry_delay * (2 ** attempt)  # æŒ‡æ•°é€€é¿
                        logger.warning(f"é‡åˆ° {e.error_type.value} é”™è¯¯ï¼Œ{wait_time}ç§’åé‡è¯•")
                        time.sleep(wait_time)
                        continue
                break
                
            except Exception as e:
                last_error = AIProcessorError(AIErrorType.UNKNOWN_ERROR, str(e), e)
                break
        
        if last_error:
            self._handle_ai_error(last_error)
        
        return None

    # def _process_with_gpt4omini(self, system_prompt, user_prompt, temperature):
    #     """ä½¿ç”¨GPT-4o-miniå¤„ç†æ–‡æœ¬"""
    #     print(f"ä½¿ç”¨GPT-4o-miniå¤„ç†æ–‡æœ¬")
    #     try:
    #         response = self.clients['openai'].chat.completions.create(
    #             messages=[
    #                 {"role": "system", "content": system_prompt},
    #                 {"role": "user", "content": user_prompt}
    #             ],
    #             model="gpt-4o-mini",
    #             temperature=temperature
    #         )
    #         return response.choices[0].message.content.strip()
    #     except Exception as e:
    #         print(f"Error with GPT-4o-mini: {e}")
    #         return None

    def _process_with_gpt4o(self, system_prompt: str, user_prompt: str, temperature: float) -> str:
        """ä½¿ç”¨GPT-4oå¤„ç†æ–‡æœ¬"""
        logger.info("ä½¿ç”¨GPT-4oå¤„ç†æ–‡æœ¬")
        
        # æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦å¯ç”¨
        if 'openai' not in self.clients:
            raise AIProcessorError(
                AIErrorType.API_KEY_MISSING,
                "OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥é…ç½®"
            )
        
        try:
            response = self.clients['openai'].chat.completions.create(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                model="gpt-4o",
                temperature=temperature,
                max_tokens=4000,  # é™åˆ¶è¾“å‡ºé•¿åº¦
                timeout=60  # è®¾ç½®è¶…æ—¶
            )
            
            if not response.choices or not response.choices[0].message.content:
                raise AIProcessorError(
                    AIErrorType.UNKNOWN_ERROR,
                    "AIè¿”å›ç©ºå“åº”"
                )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            # æ ¹æ®å¼‚å¸¸ç±»å‹è¿›è¡Œåˆ†ç±»
            error_type, message = self._classify_openai_error(e)
            raise AIProcessorError(error_type, message, e)
    
    def _classify_openai_error(self, error: Exception) -> Tuple[AIErrorType, str]:
        """åˆ†ç±»OpenAI APIé”™è¯¯"""
        error_str = str(error).lower()
        
        if "api key" in error_str or "authentication" in error_str:
            return AIErrorType.API_KEY_INVALID, "APIå¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥é…ç½®"
        
        elif "rate limit" in error_str or "quota" in error_str:
            return AIErrorType.RATE_LIMIT, "APIè°ƒç”¨é¢‘ç‡è¶…é™ï¼Œè¯·ç¨åé‡è¯•"
        
        elif "network" in error_str or "connection" in error_str or "timeout" in error_str:
            return AIErrorType.NETWORK_ERROR, "ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ"
        
        elif "model" in error_str and "not found" in error_str:
            return AIErrorType.MODEL_UNAVAILABLE, "æ¨¡å‹ä¸å¯ç”¨"
        
        elif "context length" in error_str or "too long" in error_str:
            return AIErrorType.CONTENT_TOO_LONG, "å†…å®¹è¿‡é•¿ï¼Œè¯·åˆ†å‰²å¤„ç†"
        
        else:
            return AIErrorType.UNKNOWN_ERROR, f"æœªçŸ¥é”™è¯¯: {str(error)}"
    
    def _handle_ai_error(self, error: AIProcessorError):
        """å¤„ç†AIé”™è¯¯ï¼Œæ˜¾ç¤ºç”¨æˆ·å‹å¥½çš„é”™è¯¯ä¿¡æ¯"""
        logger.error(f"AIå¤„ç†é”™è¯¯: {error.error_type.value} - {error.message}")
        
        # æ ¹æ®é”™è¯¯ç±»å‹æ˜¾ç¤ºä¸åŒçš„ç”¨æˆ·æç¤º
        if error.error_type == AIErrorType.API_KEY_MISSING:
            st.error("ğŸ”‘ APIå¯†é’¥æœªé…ç½®ï¼Œè¯·åœ¨è®¾ç½®ä¸­é…ç½®OpenAI APIå¯†é’¥")
            st.info("ğŸ’¡ æ‚¨å¯ä»¥åœ¨Streamlit Cloudçš„Secretsä¸­é…ç½®APIå¯†é’¥")
            
        elif error.error_type == AIErrorType.API_KEY_INVALID:
            st.error("âŒ APIå¯†é’¥æ— æ•ˆï¼Œè¯·æ£€æŸ¥å¯†é’¥æ˜¯å¦æ­£ç¡®")
            st.info("ğŸ’¡ è¯·ç¡®è®¤APIå¯†é’¥æ ¼å¼æ­£ç¡®ä¸”æœ‰æ•ˆ")
            
        elif error.error_type == AIErrorType.RATE_LIMIT:
            st.warning("â° APIè°ƒç”¨é¢‘ç‡è¶…é™ï¼Œè¯·ç¨åé‡è¯•")
            st.info("ğŸ’¡ å»ºè®®ç­‰å¾…å‡ åˆ†é’Ÿåå†æ¬¡å°è¯•")
            
        elif error.error_type == AIErrorType.NETWORK_ERROR:
            st.error("ğŸŒ ç½‘ç»œè¿æ¥é—®é¢˜ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
            st.info("ğŸ’¡ è¯·ç¡®è®¤ç½‘ç»œè¿æ¥æ­£å¸¸åé‡è¯•")
            
        elif error.error_type == AIErrorType.MODEL_UNAVAILABLE:
            st.error("ğŸ¤– AIæ¨¡å‹ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•")
            st.info("ğŸ’¡ æ¨¡å‹å¯èƒ½æš‚æ—¶ç»´æŠ¤ï¼Œè¯·ç¨åå†è¯•")
            
        elif error.error_type == AIErrorType.CONTENT_TOO_LONG:
            st.warning("ğŸ“„ å†…å®¹è¿‡é•¿ï¼Œè¯·å°è¯•åˆ†å‰²å¤„ç†")
            st.info("ğŸ’¡ å»ºè®®å°†PDFå†…å®¹åˆ†æˆè¾ƒå°çš„éƒ¨åˆ†å¤„ç†")
            
        else:
            st.error(f"âŒ å¤„ç†å¤±è´¥: {error.message}")
            st.info("ğŸ’¡ å¦‚é—®é¢˜æŒç»­ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒ")

    # def _process_with_deepseek(self, system_prompt, user_prompt, temperature):
    #     """ä½¿ç”¨DeepSeekå¤„ç†æ–‡æœ¬"""
    #     print(f"ä½¿ç”¨DeepSeek-V3å¤„ç†æ–‡æœ¬")
    #     try:
    #         response = self.clients['deepseek'].chat.completions.create(
    #             model="deepseek-chat",
    #             messages=[
    #                 {"role": "system", "content": system_prompt},
    #                 {"role": "user", "content": user_prompt}
    #             ],
    #             temperature=temperature
    #         )
    #         return response.choices[0].message.content.strip()
    #     except Exception as e:
    #         print(f"Error with DeepSeek: {e}")
    #         return None

    # def _process_with_claude(self,system_prompt, user_prompt):
    #     """ä½¿ç”¨Claude-3å¤„ç†æ–‡æœ¬"""
    #     print(f"ä½¿ç”¨Claude-3å¤„ç†æ–‡æœ¬")
    #     try:
    #         if 'anthropic' not in self.clients:
    #             raise ValueError("Claude API key not configured")
            
    #         prompt_tokens = self.count_tokens(user_prompt)
    #         if prompt_tokens > 100000:
    #             raise ValueError("Text exceeds Claude's context window limit")

    #         response = self.clients['anthropic'].messages.create(
    #             model="claude-3-sonnet-20240229",
    #             max_tokens=4096,
    #             system = system_prompt,
    #             messages=[{
    #                 "role": "user",
    #                 "content": user_prompt
    #             }]
    #         )
    #         return response.content[0].text
    #     except Exception as e:
    #         print(f"Error with Claude-3: {e}")
    #         return None
